<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Tim Menzies" />
  <meta name="dcterms.date" content="2024-08-22" />
  <title>How Much Data Needed for Learning?</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="ezr.css" />
  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<a href="https://github.com/timm/ezr"> <img
alt="Home" src="https://img.shields.io/badge/home-black"></a> <a href="https://raw.githubusercontent.com/timm/ezr/main/ezr.py"> <img
alt="Download" src="https://img.shields.io/badge/download-gold"></a> <a 
href="https://github.com/timm/ezr/issues"> <img
alt="Issues" src="https://img.shields.io/badge/issues-red"></a> <a 
href="https://github.com/timm/ezr/blob/main/LICENSE.md"> <img
alt="License" src="https://img.shields.io/badge/license-bsd2-green"></a> <img 
src="https://img.shields.io/badge/purpose-ai%20,%20se-blueviolet"> <img
alt="Purpose" src="https://img.shields.io/badge/language-python3-blue">

<p><em>20-40 samples can find significant improvements in 10,000+ examples. Wanna know how?</em><hr>
<header id="title-block-header">
<h1 class="title">How Much Data Needed for Learning?</h1>
<p class="author">Tim Menzies</p>
<p class="date">August 22, 2024</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#how-much" id="toc-how-much"><span
class="toc-section-number">1</span> How much?</a>
<ul>
<li><a href="#how-much-data-do-we-need-for-learning"
id="toc-how-much-data-do-we-need-for-learning"><span
class="toc-section-number">1.1</span> How Much data Do we need for
Learning?</a></li>
<li><a href="#a-more-informed-position-the-question-is-wrong"
id="toc-a-more-informed-position-the-question-is-wrong"><span
class="toc-section-number">1.2</span> A more informed position: The
question is wrong</a></li>
<li><a href="#another-question-how-much-data-can-you-handle"
id="toc-another-question-how-much-data-can-you-handle"><span
class="toc-section-number">1.3</span> Another question: How much data
can you handle?</a></li>
<li><a href="#another-question-how-much-data-can-you-get"
id="toc-another-question-how-much-data-can-you-get"><span
class="toc-section-number">1.4</span> Another question: How much data
can you get?</a></li>
<li><a href="#advice-from-mathematics"
id="toc-advice-from-mathematics"><span
class="toc-section-number">1.5</span> Advice from Mathematics</a></li>
<li><a href="#historically-how-much-data-was-enough"
id="toc-historically-how-much-data-was-enough"><span
class="toc-section-number">1.6</span> Historically, how much data was
enough?</a></li>
<li><a href="#maths" id="toc-maths"><span
class="toc-section-number">1.7</span> Maths</a>
<ul>
<li><a href="#chess-board-model" id="toc-chess-board-model"><span
class="toc-section-number">1.7.1</span> Chess board model</a></li>
<li><a href="#probable-correctness-theory"
id="toc-probable-correctness-theory"><span
class="toc-section-number">1.7.2</span> Probable Correctness
Theory</a></li>
</ul></li>
</ul></li>
<li><a href="#few-shot-learning" id="toc-few-shot-learning"><span
class="toc-section-number">2</span> # Few shot Learning</a>
<ul>
<li><a href="#few-shot-learning-in-se"
id="toc-few-shot-learning-in-se"><span
class="toc-section-number">2.0.1</span> Few Shot Learning in SE</a></li>
<li><a href="#active-learning" id="toc-active-learning"><span
class="toc-section-number">2.1</span> Active Learning</a></li>
<li><a href="#tricks-often-cheaper-faster-to-find-x-than-y"
id="toc-tricks-often-cheaper-faster-to-find-x-than-y"><span
class="toc-section-number">2.2</span> Tricks: Often cheaper, faster, to
find “X” than “Y”</a></li>
<li><a href="#se-examples-where-finding-x-is-cheaper-than-y"
id="toc-se-examples-where-finding-x-is-cheaper-than-y"><span
class="toc-section-number">2.3</span> SE Examples where finding <span
class="math inline">\(X\)</span> is cheaper than <span
class="math inline">\(Y\)</span></a></li>
<li><a href="#this-is-called-active-learning"
id="toc-this-is-called-active-learning"><span
class="toc-section-number">2.4</span> This is called “Active
Learning”</a></li>
<li><a href="#an-active-learning-loop"
id="toc-an-active-learning-loop"><span
class="toc-section-number">2.5</span> An Active Learning Loop</a></li>
<li><a href="#three-kinds-of-active-learning"
id="toc-three-kinds-of-active-learning"><span
class="toc-section-number">2.6</span> Three kinds of active
learning</a></li>
<li><a href="#methods" id="toc-methods"><span
class="toc-section-number">2.7</span> Methods</a></li>
<li><a href="#can-we-engineering-an-ai-system-simply-quickly"
id="toc-can-we-engineering-an-ai-system-simply-quickly"><span
class="toc-section-number">2.8</span> Can we engineering an AI system?
Simply? Quickly?</a></li>
<li><a href="#which-raises-the-question."
id="toc-which-raises-the-question."><span
class="toc-section-number">2.9</span> Which raises the
question….</a></li>
<li><a href="#why-study-simplicity" id="toc-why-study-simplicity"><span
class="toc-section-number">2.10</span> Why study simplicity?</a></li>
<li><a href="#why-study-simplicity-2"
id="toc-why-study-simplicity-2"><span
class="toc-section-number">2.11</span> Why study simplicity?
(2)</a></li>
<li><a href="#surfing-the-long-tail"
id="toc-surfing-the-long-tail"><span
class="toc-section-number">2.12</span> Surfing the long tail</a></li>
<li><a href="#aside-q-what-is-software-engineering"
id="toc-aside-q-what-is-software-engineering"><span
class="toc-section-number">2.13</span> Aside: Q: What is Software
Engineering?</a></li>
<li><a href="#application-of-little-data-in-se-software-review"
id="toc-application-of-little-data-in-se-software-review"><span
class="toc-section-number">2.14</span> Application of Little Data in SE:
Software Review</a></li>
<li><a href="#more-on-software-review"
id="toc-more-on-software-review"><span
class="toc-section-number">2.15</span> More on Software Review</a></li>
<li><a href="#q-how-few-questions-can-humans-answer"
id="toc-q-how-few-questions-can-humans-answer"><span
class="toc-section-number">2.16</span> Q: How few questions can humans
answer?</a></li>
<li><a href="#tricks-often-cheaper-faster-to-find-x-than-y-1"
id="toc-tricks-often-cheaper-faster-to-find-x-than-y-1"><span
class="toc-section-number">2.17</span> Tricks: Often cheaper, faster, to
find “X” than “Y”</a></li>
<li><a href="#se-examples-where-finding-x-is-cheaper-than-y-1"
id="toc-se-examples-where-finding-x-is-cheaper-than-y-1"><span
class="toc-section-number">2.18</span> SE Examples where finding <span
class="math inline">\(X\)</span> is cheaper than <span
class="math inline">\(Y\)</span></a></li>
<li><a href="#this-is-called-active-learning-1"
id="toc-this-is-called-active-learning-1"><span
class="toc-section-number">2.19</span> This is called “Active
Learning”</a></li>
<li><a href="#an-active-learning-loop-1"
id="toc-an-active-learning-loop-1"><span
class="toc-section-number">2.20</span> An Active Learning Loop</a></li>
<li><a href="#three-kinds-of-active-learning-1"
id="toc-three-kinds-of-active-learning-1"><span
class="toc-section-number">2.21</span> Three kinds of active
learning</a></li>
</ul></li>
</ul>
</nav>
<h1 data-number="1" id="how-much"><span
class="header-section-number">1</span> How much?</h1>
<h2 data-number="1.1" id="how-much-data-do-we-need-for-learning"><span
class="header-section-number">1.1</span> How Much data Do we need for
Learning?</h2>
<ul>
<li>“The Unreasonable Effectiveness of Data,” by Google’s then Chief
Scientist Peter Norvig
<ul>
<li>“Billions of trivial data points can lead to understanding” <a
href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a> (a claim he supports with numerous
examples from vision research).</li>
</ul></li>
<li>In software analytics, data-hungry researchers assume that if data
is useful, then even more data is much more useful. For example:
<ul>
<li>“..as long as it is large; the resulting prediction performance is
likely to be boosted more by the size of the sample than it is hindered
by any bias polarity that may exist” <a href="#fn2" class="footnote-ref"
id="fnref2" role="doc-noteref"><sup>2</sup></a>.</li>
<li>“It is natural to think that a closer previous release has more
similar characteristics and thus can help to train a more accurate
defect prediction model. It is also natural to think that accumulating
multiple releases can be beneficial because it represents the
variability of a project” <a href="#fn3" class="footnote-ref"
id="fnref3" role="doc-noteref"><sup>3</sup></a>.</li>
<li>“Long-term JIT models should be trained using a cache of plenty of
changes” <a href="#fn4" class="footnote-ref" id="fnref4"
role="doc-noteref"><sup>4</sup></a>.</li>
</ul></li>
</ul>
<h2 data-number="1.2"
id="a-more-informed-position-the-question-is-wrong"><span
class="header-section-number">1.2</span> A more informed position: The
question is wrong</h2>
<ul>
<li>It depends on the nature of the data
<ul>
<li>e.g. if i show you, one at a time, 100 “1”s then we are pretty sure
the 101-th thing will be “1”.</li>
<li>So if data clusters to regions with not much variance
<ul>
<li>then once we find those regions, we can stop</li>
</ul></li>
</ul></li>
<li>And sometimes its not what the data is…
<ul>
<li>.. but when we collect it</li>
<li>Shrikanth and Menzies: the early bird hypothesis
<ul>
<li>data learned from the first 150 commits builds a model as good as
anything else <a href="#fn5" class="footnote-ref" id="fnref5"
role="doc-noteref"><sup>5</sup></a>;</li>
<li>data from the early in the lifecycle is richer in examples of things
going wrong</li>
</ul></li>
</ul></li>
</ul>
<p><img width="898" alt="image" src="https://github.com/txt/aa24/assets/29195/51809d84-6c32-4d86-849d-0af40273c83f"></p>
<h2 data-number="1.3"
id="another-question-how-much-data-can-you-handle"><span
class="header-section-number">1.3</span> Another question: How much data
can you handle?</h2>
<p>For very fast decision making, there is a cognitive science case that
we work from less than a dozen examples:</p>
<ul>
<li>Larkin et al. <a href="#fn6" class="footnote-ref" id="fnref6"
role="doc-noteref"><sup>6</sup></a> characterize human expertise in
terms of very small short term memory, or STM (used as a temporary
scratch pad for current observation) and a very large long term memory,
or LTM.</li>
<li>The LTM holds separate tiny rule fragments that explore the contents
of STM to say “when you see THIS, do THAT”.</li>
<li>When an LTM rule triggers, its consequence can rewrite STM contents
which, in turn, can trigger other rules</li>
<li>Short term memory is very small, perhaps even as small as four to
seven items <a href="#fn7" class="footnote-ref" id="fnref7"
role="doc-noteref"><sup>7</sup></a> <a href="#fn8" class="footnote-ref"
id="fnref8" role="doc-noteref"><sup>8</sup></a>.</li>
<li>Experts are experts, says Larkin et al. <a href="#fn9"
class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>
because the patterns in their LTM patterns dictate what to do, without
needing to pause for reflection.</li>
<li>Novices perform worse than experts, says Larkin et al., when they
fill up their STM with too many to-do’s where they plan to pause and
reflect on what to do next.</li>
<li>Since, experts post far fewer to-do’s in their STMs, they complete
their tasks faster because (a) they are less encumbered by excessive
reflection and (b) there is more space in their STM to reason about new
information.</li>
</ul>
<p>While first proposed in 1981, this STM/LTM theory still remains
relevant <a href="#fn10" class="footnote-ref" id="fnref10"
role="doc-noteref"><sup>10</sup></a>. This theory can be used to explain
both expert competency and incompetency in software engineering tasks
such as understanding code <a href="#fn11" class="footnote-ref"
id="fnref11" role="doc-noteref"><sup>11</sup></a>.</p>
<h2 data-number="1.4"
id="another-question-how-much-data-can-you-get"><span
class="header-section-number">1.4</span> Another question: How much data
can you get?</h2>
<p>How fast can we gather expert oppinion?</p>
<ul>
<li>Some can monitor products on an assembly line, 1000s of items per
day
<ul>
<li>And there will be some error rate</li>
</ul></li>
<li>But suppose we have a panel of experts?
<ul>
<li>And suppose they have to check with everyone else before making a
decision?</li>
<li>Then everything they conclude has to be analysed, certified</li>
</ul></li>
</ul>
<p>Evidence from “cost estimation”</p>
<ul>
<li>Valerdi <a href="#fn12" class="footnote-ref" id="fnref12"
role="doc-noteref"><sup>12</sup></a> reports that panels of human
experts required three meetings (three hours each) to reach convergence
on the influence of 10 variables on 10 examples (in the domain of cost
estimation).</li>
</ul>
<p>Evidence from “Repertory Grids”</p>
<ul>
<li>A structure interview technique, learn the dimensions incrementally
as we go:
<ol type="1">
<li>Take three examples <span
class="math inline">\(E_1,E_2,E_3\)</span>:</li>
</ol>
<ul>
<li>Which one is most distant?</li>
<li>Along what dimension <span class="math inline">\(D_i\)</span> is it
distant?</li>
<li>Score those examples on this dimensions.</li>
</ul>
<ol start="2" type="1">
<li>Goto to step 1.</li>
</ol></li>
</ul>
<p><img align=600 
     src="https://csdl-images.ieeecomputer.org/mags/so/2007/02/figures/s20534.gif"></p>
<p>Advice on how long to fill in a rep grid?</p>
<ul>
<li>Kington <a href="#fn13" class="footnote-ref" id="fnref13"
role="doc-noteref"><sup>13</sup></a>:
<ul>
<li>One hour to walk thorugh <span class="math inline">\(E=16\)</span>
examples with <span class="math inline">\(D=16\)</span> dimensions.</li>
</ul></li>
<li>Easterby-Smith <a href="#fn14" class="footnote-ref" id="fnref14"
role="doc-noteref"><sup>14</sup></a>:
<ul>
<li>Keep the grid small.</li>
<li>A grid containing ten elements and ten constructs may take two hours
to complete.</li>
<li>Larger grids may take substantially more time</li>
</ul></li>
<li>Edwards’s <a href="#fn15" class="footnote-ref" id="fnref15"
role="doc-noteref"><sup>15</sup></a> rep grid interview sessions lasted,
on average, for one hour (although the shortest was 30 min) and these
were recorded for later analysis.</li>
</ul>
<p>Overall, we get, for reflective labels on data:</p>
<ul>
<li>10 to 20 examples per 1-4 hours</li>
</ul>
<h2 data-number="1.5" id="advice-from-mathematics"><span
class="header-section-number">1.5</span> Advice from Mathematics</h2>
<p>One commonly cited rule of thumb [^call] is to have at least 10 times
the number of training data instances attributes <a href="#fn16"
class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a>
<a href="#fn17" class="footnote-ref" id="fnref17"
role="doc-noteref"><sup>17</sup></a>.</p>
<ul>
<li>by this rule, 20 attributes needs 200 rows</li>
</ul>
<h2 data-number="1.6" id="historically-how-much-data-was-enough"><span
class="header-section-number">1.6</span> Historically, how much data was
enough?</h2>
<ul>
<li>Semi-supervised learning: <span
class="math inline">\(\sqrt{N}\)</span> is enough
<ul>
<li>Alvarez <a href="#fn18" class="footnote-ref" id="fnref18"
role="doc-noteref"><sup>18</sup></a> recursively bi-cluster down to
<span class="math inline">\(\sqrt{N}\)</span> then collect one data
point per leaf cluster.</li>
</ul></li>
</ul>
<ul>
<li>Zhu et al. <a href="#fn19" class="footnote-ref" id="fnref19"
role="doc-noteref"><sup>19</sup></a>
<ul>
<li>Numerous examples from facila recongition where face accuracy
plateaus at <span class="math inline">\(M&gt; 100\)</span> examples</li>
</ul></li>
<li>Incremental learning in SE: <a href="#fn20" class="footnote-ref"
id="fnref20" role="doc-noteref"><sup>20</sup></a>
<ul>
<li>Sort data on historical order;</li>
<li>Train up to time <span class="math inline">\(t\)</span>, test of
data after <span class="math inline">\(t\)</span>.</li>
<li>Even with 1000s of examples
<ul>
<li>Learning on 25 defective examples + 25 non-defective</li>
<li>Did as good as anything else.</li>
</ul></li>
</ul></li>
</ul>
<h2 data-number="1.7" id="maths"><span
class="header-section-number">1.7</span> Maths</h2>
<h3 data-number="1.7.1" id="chess-board-model"><span
class="header-section-number">1.7.1</span> Chess board model</h3>
<p>Data is spread out acoss a d-dimensional chessboard where each
dimension is divided into <span class="math inline">\(b\)</span> bins <a
href="#fn21" class="footnote-ref" id="fnref21"
role="doc-noteref"><sup>21</sup></a>.</p>
<ul>
<li>Data is skewed by some factor <span class="math inline">\(k\)</span>
such that not all cells have data.</li>
<li>a standard chess board is <span
class="math inline">\(d,b=2,8\)</span></li>
</ul>
<p>The target is some subset of the data that falls into some of the
chessboard cells:</p>
<ul>
<li><span class="math inline">\(c=b^{-d}\)</span> is the probability of
picking a particular cell;</li>
<li><span class="math inline">\(C=1-(1-p)^n\)</span> is the certaintity
that once we arrive at a cells, that we can find the signal after <span
class="math inline">\(n\)</span> attempts (which occurs at probability
<span class="math inline">\(p\)</span> in that cell)</li>
<li>So lets do some simulations. 1,000 times pick at random from the
following:
<ul>
<li><span class="math inline">\(k \in \{ 1, 2, 3, 4, 5\}\)</span></li>
<li><span class="math inline">\(d \in \{ 3, 4, 5, 6, 7\}\)</span>
dimensions;</li>
<li><span class="math inline">\(b \in \{ 2 , 3, 4, 5, 6, 7\}\)</span>
bins;</li>
<li><span class="math inline">\(p \in \{0.1, 0.2, 0,3,
0.4\}\)</span></li>
<li>We gave the whole thing to a decision tree learner, asking what
factors predicts for more than 67% chance of finding the target.
<ul>
<li>and the learnt tree told us:
<ul>
<li>We need up to 200 examples when the defect signal is rare (<span
class="math inline">\(p\le 10\)</span> percent)</li>
<li>But far fewer when the signal occurs at <span
class="math inline">\(p &gt; 10\)</span> percent.</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h3 data-number="1.7.2" id="probable-correctness-theory"><span
class="header-section-number">1.7.2</span> Probable Correctness
Theory</h3>
<p>Richard Hamlet, Proable correctness theory, 1980 <a href="#fn22"
class="footnote-ref" id="fnref22"
role="doc-noteref"><sup>22</sup></a>.</p>
<ul>
<li>Confidence <span class="math inline">\(C\)</span> that we will see
an event at probability <span class="math inline">\(p\)</span> after
<span class="math inline">\(n\)</span> random trials:
<ul>
<li><span class="math inline">\(C(p,n) =1-(1-p)^n\)</span></li>
</ul></li>
<li>Which re-arranges to:
<ul>
<li><span class="math inline">\(n(C,p) =
\frac{\log(1-C)}{\log(1-p)}\)</span></li>
</ul></li>
<li>And if we have a sorting trick to divide examples into <em>best</em>
and <em>rest</em>
<ul>
<li>then the number of evals is <span
class="math inline">\(\log_2(n(C,p))\)</span></li>
</ul></li>
</ul>
<p>Some what ifs: - If we apply Cohen’s rule (things are
indistinguishable if less than <span
class="math inline">\(d{\times}\sigma\)</span> apart, - And if variables
are Gaussian ranging <span class="math inline">\(-3 \le x \le
3\)</span>. - Then that space divides into regions of size <span
class="math inline">\(p=\frac{d}{6}\)</span></p>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 20%" />
<col style="width: 36%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">scenario</th>
<th style="text-align: left;">d</th>
<th style="text-align: left;">p</th>
<th style="text-align: left;">C</th>
<th style="text-align: left;">n(c,p)</th>
<th style="text-align: left;"><span
class="math inline">\(\log_2(n(c,p))\)</span></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">medium effect, non-safety critical</td>
<td style="text-align: left;">0.35</td>
<td style="text-align: left;">0.06</td>
<td style="text-align: left;">0.95</td>
<td style="text-align: left;">50</td>
<td style="text-align: left;">6</td>
</tr>
<tr>
<td style="text-align: left;">small effect, safety criticali</td>
<td style="text-align: left;">0.2</td>
<td style="text-align: left;">0.03</td>
<td style="text-align: left;">0.9999</td>
<td style="text-align: left;">272</td>
<td style="text-align: left;">8</td>
</tr>
<tr>
<td style="text-align: left;">tiny effects, ultra-safety critical</td>
<td style="text-align: left;">n/a</td>
<td style="text-align: left;">one in a million</td>
<td style="text-align: left;">six sigma<br> (0.999999)</td>
<td style="text-align: left;">13,815,504</td>
<td style="text-align: left;">24</td>
</tr>
</tbody>
</table>
<p>Note the above table makes some very optimistic assumptions about the
problem:</p>
<ul>
<li>It is single variable gaussian</li>
<li>Solutions are spaced equally across the x-axis</li>
</ul>
<p>But it also tells us that the only way we can reason about safety
critical systems is via some sorting heuristic (so we can get the log2
effect) [^call]: Application of machine learning techniques in small
sample clinical studies, from StackExchange.com
https://stats.stackexchange.com/questions/1856/application-of-machine-learning-techniques-in-small-sample-clinical-studies</p>
<h1 data-number="2" id="few-shot-learning"><span
class="header-section-number">2</span> # Few shot Learning</h1>
<p>In the following, the author says <strong>LLMs</strong> not
<strong>learners</strong> but given the results of this subject, I think
an edit is in order:</p>
<ul>
<li>A particularly exciting aspect of <strong>learners</strong> is their
knack for few-shot and zero-shot learning: they can learn to perform a
task with very few examples. Few- shotting has particular synergies in
software engineering, where there are a lot of phenomena (identifier
names, APIs, terminology, coding patterns) that are known to be highly
project-specific. However, project-specific data can be quite limited,
especially early in the history of a project; thus the few-shot learning
capacity of <strong>learners</strong> might be very relevant.</li>
<li>In the extreme case, training data consists of only one instance for
each target class, which is known as one-shot learning</li>
</ul>
<p>Need another name</p>
<ul>
<li>Few-shot learning is usually a neural model concept
<ul>
<li>Uses a large language models trained on massive code corpora</li>
<li>Generalize to new tasks via a sequence of prompts, starting composed
of natural language instructions, then few instances of task
demonstration, and a query</li>
</ul></li>
<li>Few-shot learning (version 2.0)
<ul>
<li>takes no model as an input (but it could be initialized from a model
learned at a prior session)</li>
<li>Generalize to new tasks via a sequence of queries to the data,
selected by the model built so far.<br />
</li>
</ul></li>
<li>Perhaps it might be useful to combine SMO and few-shot into another
term</li>
</ul>
<p>Generalize to new tasks via a sequence of prompts, starting composed
of natural language instructions,</p>
<ul>
<li>Data-Efficient Sequential Learning (DESL)
<ul>
<li>Sequential Decision-Making : how to making decisions one after
another, where each decision is based on prior outcomes.</li>
<li>Data Selection Strategy : how to selecting which data points to
label or otherwise interact with next. In SMO, this is the acquire
function.</li>
<li>Model Adaptivity : how to update your models for SDM and/or DSS as a
result of the interaction seen in DSS</li>
<li>Label Efficiency : concerns the cost of labelling. DESL is least
useful when the label effeciency is high.</li>
</ul></li>
<li>Other analaogous terms include active learnin which in statistics
literature also called optimal experimental design or query learning, is
a class of strategies to choose the data from which to learn.</li>
</ul>
<p>Few-shot learning is a subfield of machine learning and deep learning
that aims to teach AI models how to learn from only a small number of
labeled training data.</p>
<p>More generally “n-shot learning” a category of artificial
intelligence that also includes:</p>
<ul>
<li>one-shot learning (in which there is only one labeled example of
each class to be learned)</li>
<li>and zero-shot learning (in which there are no labeled examples at
all</li>
</ul>
<p>Applications:</p>
<ul>
<li>Healthcare: rarity of certain conditions or the expertise required
to accurately annotate medical data (like MRIs or echocardiography) can
make the acquisition of a large number of labeled samples prohibitively
difficult</li>
<li>Robotics: enable robots to quickly adapt to new environments</li>
<li>SE: Any learning task when labels are short: e.g. what tests to run
next, etc etc</li>
</ul>
<p>Methods:</p>
<ul>
<li>transfer learning (use knowledge from elswhere)
<ul>
<li>adapt a pre-trained model</li>
<li>e.g. for us in SMO, intialize our best,rest models from prior
applications
<ul>
<li>this can be very useful since, rarely, do we ever do things
once</li>
<li>if it is important, we will do it again and again and again…</li>
</ul></li>
</ul></li>
<li>Data-level approach
<ul>
<li>automatically generate additional training samples</li>
<li>Data generation
<ul>
<li>GANs (generative adaptive networks)
<ul>
<li>Two models discrimninator and adversay</li>
<li>Model1 produces</li>
<li>Model2 classifies as right,wrong</li>
<li>Model1 stops when model2 can no long distinguish good from bad</li>
<li>Feedback from one model can be used to guide the other</li>
<li>And if you thought one deep learner was slow, try running two</li>
</ul></li>
</ul></li>
<li>Data augmentation
<ul>
<li>Find ways to add in the missing bits</li>
<li>Semi-supervised learning e.g. RRP, label once per leaf, share the
label across entire leaf</li>
<li>Active learning; e.g. SMO, only ask for labels on most informative
examples.</li>
</ul></li>
</ul></li>
</ul>
<h3 data-number="2.0.1" id="few-shot-learning-in-se"><span
class="header-section-number">2.0.1</span> Few Shot Learning in SE</h3>
<p>March 2024: Google query: “few-shot learning and ‘software
engineering’”</p>
<p>In the first 100 returns, after paper70, no more published few shot
learning papers in SE.</p>
<p>In the remaining 70 papers:</p>
<table style="width:100%;">
<colgroup>
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr>
<th>year</th>
<th>citations</th>
<th>venue</th>
<th>j=journal; <br>c=conf;<nr> w=workshop</th>
<th>title</th>
<th>pdf</th>
<th>data</th>
</tr>
</thead>
<tbody>
<tr>
<td>2023</td>
<td>1</td>
<td>Icse_NLBSE</td>
<td>w</td>
<td>Few-Shot Learning for Issue Report Classification</td>
<td><a
href="Few-Shot_Learning_for_Issue_Report_Classification.pdf">pdf</a></td>
<td>200 + 200</td>
</tr>
<tr>
<td>2023</td>
<td>2</td>
<td>SSBSE</td>
<td>c</td>
<td>. Search-based Optimisation of LLM Learning Shots for Story Point
Estimation</td>
<td><a href="SB_LLM_Shot_optimisation.pdf">pdf</a></td>
<td>6 to 10</td>
</tr>
<tr>
<td>2023</td>
<td>2</td>
<td>ICSE</td>
<td>c</td>
<td>Log Parsing with Prompt-based Few-shot Learning</td>
<td><a
href="Log_Parsing_with_Prompt-based_Few-shot_Learning.pdf">pdf</a></td>
<td>4 to 128. most improvement before 16</td>
</tr>
<tr>
<td>2023</td>
<td>3</td>
<td>AST</td>
<td>c</td>
<td>FlakyCat: Predicting Flaky Tests Categories using Few-Shot
Learning</td>
<td><a
href="FlakyCat_Predicting_Flaky_Tests_Categories_using_Few-Shot_Learning.pdf">pdf</a></td>
<td>400+</td>
</tr>
<tr>
<td>2023</td>
<td>5</td>
<td>ICSE</td>
<td>c</td>
<td>Retrieval-Based Prompt Selection for Code-Related Few-Shot
Learning</td>
<td><a
href="Retrieval-Based_Prompt_Selection_for_Code-Related_Few-Shot_Learning.pdf">pdf</a></td>
<td>6-7 (for code generation (40 to 50 (for code repair)</td>
</tr>
<tr>
<td>2022</td>
<td>7</td>
<td>Soft.Lang.Eng</td>
<td>c</td>
<td>Neural Language Models and Few Shot Learning for Systematic
Requirements Processing in MDSE</td>
<td><a href="3567512.3567534.pdf">pdf</a></td>
<td>8 to 11</td>
</tr>
<tr>
<td>2023</td>
<td>12</td>
<td>ICSE</td>
<td>c</td>
<td>Towards using Few-Shot Prompt Learning for Automating Model
Completion</td>
<td><a
href="Towards_using_Few-Shot_Prompt_Learning_for_Automating_Model_Completion.pdf">pdf</a></td>
<td>212 classes</td>
</tr>
<tr>
<td>2020</td>
<td>15</td>
<td>IEEE ACCECSS</td>
<td>j</td>
<td>Few-Shot Learning Based Balanced Distribution Adaptation for
Heterogeneous Defect Prediction</td>
<td><a
href="Few-Shot_Learning_Based_Balanced_Distribution_Adaptation_for_Heterogeneous_Defect_Prediction.pdf">pdf</a></td>
<td>100s - 1000s</td>
</tr>
<tr>
<td>2019</td>
<td>21</td>
<td>Big Data</td>
<td>j</td>
<td>. Exploring the applicability of low-shot learning in mining
software repositories</td>
<td><a href="s40537-019-0198-z.pdf">pdf</a></td>
<td>100 =&gt;70% accuracy; 100s ==&gt; 90% accuracy</td>
</tr>
<tr>
<td>2021</td>
<td>27</td>
<td>ESEM</td>
<td>c</td>
<td>An Empirical Examination of the Impact of Bias on Just-in-time
Defect Prediction</td>
<td></td>
<td>10^3 samples of defects</td>
</tr>
<tr>
<td>2020</td>
<td>29</td>
<td>ICSE</td>
<td>c</td>
<td>Unsuccessful Story about Few Shot Malware Family Classification and
Siamese Network to the Rescue</td>
<td><a href="3377811.3380354.pdf">pdf</a></td>
<td>10,000s ?</td>
</tr>
<tr>
<td>2022</td>
<td>65</td>
<td>ASE</td>
<td>c</td>
<td>Few-shot training LLMs for project-specific code-summarization</td>
<td><a href="3551349.3559555.pdf">pdf</a></td>
<td>10 samples</td>
</tr>
<tr>
<td>2022</td>
<td>101</td>
<td>FSE</td>
<td>c</td>
<td>Less Training_ More Repairing Please: Revisiting Automated Program
Repair via Zero-Shot Learning</td>
<td><a href="3540250.3549101.pdf">pdf</a></td>
<td>?</td>
</tr>
</tbody>
</table>
<h2 data-number="2.1" id="active-learning"><span
class="header-section-number">2.1</span> Active Learning</h2>
<ul>
<li>Assumes abundant access to examples,
<ul>
<li>but very limited access to oracles that can label each example</li>
</ul></li>
<li>A repeated result:
<ul>
<li>Learners learn better (using fewer labels)</li>
<li>if they can decode for them selves what examples to use.</li>
</ul></li>
<li>Goal:
<ul>
<li>build a competent model using as few labels as followed.</li>
</ul></li>
</ul>
<h2 data-number="2.2"
id="tricks-often-cheaper-faster-to-find-x-than-y"><span
class="header-section-number">2.2</span> Tricks: Often cheaper, faster,
to find “X” than “Y”</h2>
<ul>
<li><span class="math inline">\(Y=f(X)\)</span>
<ul>
<li><span class="math inline">\(X\)</span>,<span
class="math inline">\(Y\)</span> are our independent and dependent
variables.</li>
<li><span class="math inline">\(f\)</span> is the thing we are trying to
find</li>
</ul></li>
<li>e.g. Fishing:
<ul>
<li>Glance up and down the river.</li>
<li>That looks like a good spot.</li>
<li>3 hours later: well, it was not</li>
</ul></li>
<li>e.g. Used car yard:
<ul>
<li>Glancing over 100 cars: count the cars and their colors and number
of wheels and size of car.</li>
<li>But to find gas mileage– got to take each out for a long drive.</li>
</ul></li>
</ul>
<h2 data-number="2.3"
id="se-examples-where-finding-x-is-cheaper-than-y"><span
class="header-section-number">2.3</span> SE Examples where finding <span
class="math inline">\(X\)</span> is cheaper than <span
class="math inline">\(Y\)</span></h2>
<ul>
<li><span class="math inline">\(X\)</span>,<span
class="math inline">\(Y\)</span> are our independent and dependent
variables.</li>
<li>Quick to mine <span class="math inline">\(X\)</span> GitHub to get
code size, dependencies per function,
<ul>
<li>Slow to get <span class="math inline">\(Y\)</span> (a) development
time, (b) what people will pay for it</li>
</ul></li>
<li>Quick to count <span class="math inline">\(X\)</span> the number of
classes in a system.
<ul>
<li>Slow to get <span class="math inline">\(Y\)</span> an organization
to tell you human effort to build and maintain that code.</li>
</ul></li>
<li>Quick to enumerate <span class="math inline">\(X\)</span> many
design options (20 yes-no = <span class="math inline">\(2^{20}\)</span>
options)
<ul>
<li>Slow to check <span class="math inline">\(Y\)</span> those options
with the human stakeholders.</li>
</ul></li>
<li>Quick to list <span class="math inline">\(X\)</span> configuration
parameters for the software.
<ul>
<li>Slow to find <span class="math inline">\(X\)</span> runtime and
energy requirements for all configurations.</li>
</ul></li>
<li>Quick to list <span class="math inline">\(X\)</span> data miner
params (e.g. how many neighbors in knn?)
<ul>
<li>Slow to find <span class="math inline">\(Y\)</span> best setting for
local data.</li>
</ul></li>
<li>Quick to make <span class="math inline">\(X\)</span> test case
inputs using (e.g.) random input selection
<ul>
<li>Slow to run all tests and get <span class="math inline">\(Y\)</span>
humans to check each output</li>
</ul></li>
</ul>
<h2 data-number="2.4" id="this-is-called-active-learning"><span
class="header-section-number">2.4</span> This is called “Active
Learning”</h2>
<ul>
<li>Learning works better if the learner can pick its training
data[^brochu]. Given two models that predict for good <span
class="math inline">\(g\)</span> or bad <span
class="math inline">\(b\)</span>:</li>
<li>An active learning loop:</li>
</ul>
<h2 data-number="2.5" id="an-active-learning-loop"><span
class="header-section-number">2.5</span> An Active Learning Loop</h2>
<ul>
<li><em>Labelling</em>: given an example with <span
class="math inline">\(X\)</span>, but not <span
class="math inline">\(Y\)</span>, get the <span
class="math inline">\(Y\)</span>.</li>
<li>Just for simplicity, assume we a model can inputs <span
class="math inline">\(X\)</span> values to predict for good <span
class="math inline">\(g\)</span> or bad <span
class="math inline">\(b\)</span>:</li>
</ul>
<table>
<colgroup>
<col style="width: 15%" />
<col style="width: 38%" />
<col style="width: 46%" />
</colgroup>
<thead>
<tr>
<th style="text-align: right;">n</th>
<th>Task</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">1</td>
<td>Sample a little</td>
<td>Get a get a few <span class="math inline">\(Y\)</span> values
(picked at random?)</td>
</tr>
<tr>
<td style="text-align: right;">2</td>
<td>Learn a little</td>
<td>Build a tiny model from that sample</td>
</tr>
<tr>
<td style="text-align: right;">3</td>
<td>Reflect</td>
<td>Compute <span class="math inline">\(b,r\)</span></td>
</tr>
<tr>
<td style="text-align: right;">4</td>
<td>Acquire</td>
<td>Label an example that (e.g.) maximizes <span
class="math inline">\(b/r\)</span>. Add it to the sample</td>
</tr>
<tr>
<td style="text-align: right;">5</td>
<td>Repeat</td>
<td>Goto 2</td>
</tr>
</tbody>
</table>
<p>How to - Sample, once - Use reflection to find one unlabelled
thingFind &amp;$ the <span class="math inline">\(X\)</span> variables, -
guess what might be the next most informative example - get its <span
class="math inline">\(Y\)</span> value, .</p>
<h2 data-number="2.6" id="three-kinds-of-active-learning"><span
class="header-section-number">2.6</span> Three kinds of active
learning</h2>
<h2 data-number="2.7" id="methods"><span
class="header-section-number">2.7</span> Methods</h2>
<ul>
<li>Many and varied</li>
<li>Here’s my “SMO” (sequential model optimization): one candidate
strategy (explored by co-PI Menzies~, amongst others) is:
This loop is useful for reducing the cognitive load on humans, in two
ways:
Aside: when showing code to a user, “reducing the number of features
shown” means “controlling what information is highlighted” from the code
snippet.<br />
Using code summarizing tools, we could infer examples (to be shown to
the user) that are much smaller than the actual snippets. For example,
using Markov chain frequencies for the tokens in the code, we could
summarize by de-emphasize parts of the code that have the same
distributions in the poisoned/non-poisoned code samples.</li>
</ul>
Note some implementation details:
<ul>
<li>Do it simpler, faster. using fewer resources?</li>
<li>Know how to combine things, such that you can more with less?</li>
<li>Teach seemingly complex things to newbies?</li>
</ul>
<h2 data-number="2.8"
id="can-we-engineering-an-ai-system-simply-quickly"><span
class="header-section-number">2.8</span> Can we engineering an AI
system? Simply? Quickly?</h2>
<p>Here we explore dozens of SE problems <a href="#fn23"
class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a>
using explainable AI for semi-supervised multi-objective
optimization.</p>
<ul>
<li>Internally, this is coded via sequential model optimization and
membership query synthesis.</li>
</ul>
<p>Sounds complex, right?</p>
<ul>
<li>But it ain’t.</li>
<li>In fact, as I hope to show, all the above is just a hundred lines of
code (caveat: if you are using the right underlying object model).</li>
</ul>
<h2 data-number="2.9" id="which-raises-the-question."><span
class="header-section-number">2.9</span> Which raises the
question….</h2>
<p>What else is similarly simple?</p>
<ul>
<li>How many of our complex problems … aren’t?</li>
</ul>
<p>My challenge to you is this:</p>
<ul>
<li>Please go and find out.</li>
<li>Take a working system, see what you can throw away (while the
remaining system is still useful and fast).</li>
<li>Let me know happens so I can add your fantastic new, and simple,
idea to this code</li>
</ul>
<h2 data-number="2.10" id="why-study-simplicity"><span
class="header-section-number">2.10</span> Why study simplicity?</h2>
<ul>
<li>Cause we are getting really really good at reasoning with very
little data</li>
<li>Cause its good science
<ul>
<li>If you really understand “it”, can you do “it” again, very very
simply</li>
</ul></li>
<li>Cause the world is changing
<ul>
<li>Next generation of satellite internet providers</li>
<li>Connecting millions of new programmers willing to work for <span
class="math inline">\(\frac{1}{20}\)</span>th of the salary you
want</li>
<li>In that world, you do not want to be the programmer</li>
<li>You want to be the optimizer who controls and improves the work of
others.</li>
</ul></li>
<li>Cause almost no one else is studying reasoning with very little
data</li>
</ul>
<h2 data-number="2.11" id="why-study-simplicity-2"><span
class="header-section-number">2.11</span> Why study simplicity? (2)</h2>
<ul>
<li>Cause everyone has gone mad on complexity.
<ul>
<li>A small number of very large companies have built empires based on
“big data”</li>
<li>Five years ago, no on one wanted to head about simplicity</li>
<li>But after three years of constant tech lay offs, and increasing
challenges for getting jobs at these large organizations …</li>
<li>… my students now know they need to graduate with knowledge about
“big data” AND alternate approaches.</li>
</ul></li>
<li>Cause big data is running out of data (see next slide).</li>
</ul>
<h2 data-number="2.12" id="surfing-the-long-tail"><span
class="header-section-number">2.12</span> Surfing the long tail</h2>
<ul>
<li>LLMs? For everything?
<ul>
<li>LLMs know a lot, about things we do a lot (e.g. “if” statements in
code)</li>
<li>And they know <em>less</em> about things we do <em>less</em>
often</li>
</ul></li>
<li>Model collapse:
<ul>
<li>We are about to run out of training data <a href="#fn24"
class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a>
for LLMs.</li>
<li>Can’t reply of synthetic data generation (no new information from
data already seen)</li>
</ul></li>
<li>So, we make do with <em>less</em> data?
<ul>
<li>Are their domain, were models need less, not more, data?</li>
</ul></li>
</ul>
<h2 data-number="2.13" id="aside-q-what-is-software-engineering"><span
class="header-section-number">2.13</span> Aside: Q: What is Software
Engineering?</h2>
<ul>
<li>A: The delivery and maintainable of software products to an
acceptable standard, built using current constraints.</li>
<li>LLMs build “quality” solutions? That respect “current
constraints”?</li>
<li>Do LLMs work for all tasks?
<ul>
<li>Not really<a href="#fn25" class="footnote-ref" id="fnref25"
role="doc-noteref"><sup>25</sup></a>. They often produce useful
suggestions. But mixed in with the good is also the bad, the terrible,
the misleading and the dangerous<a href="#fn26" class="footnote-ref"
id="fnref26" role="doc-noteref"><sup>26</sup></a>.</li>
</ul></li>
<li>Are LLMs bad science?
<ul>
<li>Harder for research LLM papers to, say, run 20 times and report the
variability in the results</li>
<li>Harder for other researchers to check results from other
people.</li>
<li>AI needs 1% of world’s power, creating 4% of our carbon emissions<a
href="#fn27" class="footnote-ref" id="fnref27"
role="doc-noteref"><sup>27</sup></a></li>
</ul></li>
<li>Do they need too much data ? (above)</li>
<li>Are there other options? That use less data? That can be
tested?</li>
</ul>
<h2 data-number="2.14"
id="application-of-little-data-in-se-software-review"><span
class="header-section-number">2.14</span> Application of Little Data in
SE: Software Review</h2>
<p><img src="./img/review.png" width="300" /></p>
<ul>
<li>The more we use AI in SE, the more code will be auto-generated.</li>
<li>The more we auto-generate code
<ul>
<li>the less time software engineers spend writing and reviewing new
code, written by someone or something else (which internally, are a
mystery)</li>
</ul></li>
<li>The less we understand code,
<ul>
<li>the more we will use black-boxes components, where, once a system is
assembled, its control settings are tuned.</li>
</ul></li>
</ul>
<p>In this scenario: we must reduce the effort (human and CPU) for that
tuning.</p>
<h2 data-number="2.15" id="more-on-software-review"><span
class="header-section-number">2.15</span> More on Software Review</h2>
<ul>
<li>We define “software review” as a panel of SMEs (subject matter
experts), looking at examples of behavior to recommend how to improve
software.</li>
<li>SME time is usually very limited so, such reviews must complete
after looking at just a small number of very informative examples.</li>
<li>To support the software review process, we explore methods that
train a predictive model to guess if some oracle will like/dislike the
next example.</li>
<li>These predictive models work with SMEs to guide them as they explore
the examples. Afterwards, the models can handle new examples, while the
panelists are busy, elsewhere</li>
</ul>
<h2 data-number="2.16" id="q-how-few-questions-can-humans-answer"><span
class="header-section-number">2.16</span> Q: How few questions can
humans answer?</h2>
<p>A: Not so many</p>
<table>
<colgroup>
<col style="width: 38%" />
<col style="width: 61%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">What</th>
<th style="text-align: left;">N</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Standard theory</td>
<td style="text-align: left;">more is always better</td>
</tr>
<tr>
<td style="text-align: left;">Cognitive Science</td>
<td style="text-align: left;">7 plus or minus 2</td>
</tr>
<tr>
<td style="text-align: left;">From human studies (cost estimation, rep
grids)<a href="#fn28" class="footnote-ref" id="fnref28"
role="doc-noteref"><sup>28</sup></a></td>
<td style="text-align: left;">10 to 20 examples per 1-4 hours</td>
</tr>
<tr>
<td style="text-align: left;">Regression <a href="#fn29"
class="footnote-ref" id="fnref29"
role="doc-noteref"><sup>29</sup></a></td>
<td style="text-align: left;">10-20 examples per attribute</td>
</tr>
<tr>
<td style="text-align: left;">Semi-supervised learning</td>
<td style="text-align: left;"><span
class="math inline">\(\sqrt{N}\)</span></td>
</tr>
<tr>
<td style="text-align: left;">Zhu et al.<a href="#fn30"
class="footnote-ref" id="fnref30"
role="doc-noteref"><sup>30</sup></a></td>
<td style="text-align: left;">100 images</td>
</tr>
<tr>
<td style="text-align: left;">Menzies et al. 2008<a href="#fn31"
class="footnote-ref" id="fnref31"
role="doc-noteref"><sup>31</sup></a></td>
<td style="text-align: left;">50 examples</td>
</tr>
<tr>
<td style="text-align: left;">Chessboard model<a href="#fn32"
class="footnote-ref" id="fnref32"
role="doc-noteref"><sup>32</sup></a></td>
<td style="text-align: left;">200 examples</td>
</tr>
<tr>
<td style="text-align: left;">Probable Correctness theory<a href="#fn33"
class="footnote-ref" id="fnref33"
role="doc-noteref"><sup>33</sup></a></td>
<td style="text-align: left;">simpler cases: 50 to 6 (if binary
chop)</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">safety-critical cases: 272 to 8 (if binary
chop)</td>
</tr>
</tbody>
</table>
<h2 data-number="2.17"
id="tricks-often-cheaper-faster-to-find-x-than-y-1"><span
class="header-section-number">2.17</span> Tricks: Often cheaper, faster,
to find “X” than “Y”</h2>
<ul>
<li><span class="math inline">\(Y=f(X)\)</span>
<ul>
<li><span class="math inline">\(X\)</span>,<span
class="math inline">\(Y\)</span> are our independent and dependent
variables.</li>
<li><span class="math inline">\(f\)</span> is the thing we are trying to
find</li>
</ul></li>
<li>e.g. Fishing:
<ul>
<li>Glance up and down the river.</li>
<li>That looks like a good spot.</li>
<li>3 hours later: well, it was not</li>
</ul></li>
<li>e.g. Used car yard:
<ul>
<li>Glancing over 100 cars: count the cars and their colors and number
of wheels and size of car.</li>
<li>But to find gas mileage– got to take each out for a long drive.</li>
</ul></li>
</ul>
<h2 data-number="2.18"
id="se-examples-where-finding-x-is-cheaper-than-y-1"><span
class="header-section-number">2.18</span> SE Examples where finding
<span class="math inline">\(X\)</span> is cheaper than <span
class="math inline">\(Y\)</span></h2>
<ul>
<li><span class="math inline">\(X\)</span>,<span
class="math inline">\(Y\)</span> are our independent and dependent
variables.</li>
<li>Quick to mine <span class="math inline">\(X\)</span> GitHub to get
code size, dependencies per function,
<ul>
<li>Slow to get <span class="math inline">\(Y\)</span> (a) development
time, (b) what people will pay for it</li>
</ul></li>
<li>Quick to count <span class="math inline">\(X\)</span> the number of
classes in a system.
<ul>
<li>Slow to get <span class="math inline">\(Y\)</span> an organization
to tell you human effort to build and maintain that code.</li>
</ul></li>
<li>Quick to enumerate <span class="math inline">\(X\)</span> many
design options (20 yes-no = <span class="math inline">\(2^{20}\)</span>
options)
<ul>
<li>Slow to check <span class="math inline">\(Y\)</span> those options
with the human stakeholders.</li>
</ul></li>
<li>Quick to list <span class="math inline">\(X\)</span> configuration
parameters for the software.
<ul>
<li>Slow to find <span class="math inline">\(X\)</span> runtime and
energy requirements for all configurations.</li>
</ul></li>
<li>Quick to list <span class="math inline">\(X\)</span> data miner
params (e.g. how many neighbors in knn?)
<ul>
<li>Slow to find <span class="math inline">\(Y\)</span> best setting for
local data.</li>
</ul></li>
<li>Quick to make <span class="math inline">\(X\)</span> test case
inputs using (e.g.) random input selection
<ul>
<li>Slow to run all tests and get <span class="math inline">\(Y\)</span>
humans to check each output</li>
</ul></li>
</ul>
<h2 data-number="2.19" id="this-is-called-active-learning-1"><span
class="header-section-number">2.19</span> This is called “Active
Learning”</h2>
<ul>
<li>Learning works better if the learner can pick its training
data[^brochu]. Given two models that predict for good <span
class="math inline">\(g\)</span> or bad <span
class="math inline">\(b\)</span>:</li>
<li>An active learning loop:</li>
</ul>
<h2 data-number="2.20" id="an-active-learning-loop-1"><span
class="header-section-number">2.20</span> An Active Learning Loop</h2>
<ul>
<li><em>Labelling</em>: given an example with <span
class="math inline">\(X\)</span>, but not <span
class="math inline">\(Y\)</span>, get the <span
class="math inline">\(Y\)</span>.</li>
<li>Just for simplicity, assume we a model can inputs <span
class="math inline">\(X\)</span> values to predict for good <span
class="math inline">\(g\)</span> or bad <span
class="math inline">\(b\)</span>:</li>
</ul>
<table>
<colgroup>
<col style="width: 15%" />
<col style="width: 38%" />
<col style="width: 46%" />
</colgroup>
<thead>
<tr>
<th style="text-align: right;">n</th>
<th>Task</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">1</td>
<td>Sample a little</td>
<td>Get a get a few <span class="math inline">\(Y\)</span> values
(picked at random?)</td>
</tr>
<tr>
<td style="text-align: right;">2</td>
<td>Learn a little</td>
<td>Build a tiny model from that sample</td>
</tr>
<tr>
<td style="text-align: right;">3</td>
<td>Reflect</td>
<td>Compute <span class="math inline">\(b,r\)</span></td>
</tr>
<tr>
<td style="text-align: right;">4</td>
<td>Acquire</td>
<td>Label an example that (e.g.) maximizes <span
class="math inline">\(b/r\)</span>. Add it to the sample</td>
</tr>
<tr>
<td style="text-align: right;">5</td>
<td>Repeat</td>
<td>Goto 2</td>
</tr>
</tbody>
</table>
<p>How to - Sample, once - Use reflection to find one unlabelled
thingFind &amp;$ the <span class="math inline">\(X\)</span> variables, -
guess what might be the next most informative example - get its <span
class="math inline">\(Y\)</span> value, .</p>
<h2 data-number="2.21" id="three-kinds-of-active-learning-1"><span
class="header-section-number">2.21</span> Three kinds of active
learning</h2>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>P. Norvig. (2011) The Unreasonable Effectiveness of
Data. Youtube. https://www.youtube.com/watch?v=yvDCzhbjYWs<a
href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>F. Rahman, D. Posnett, I. Herraiz, and P. Devanbu,
“Sample size vs. bias in defect prediction,” in Proceedings of the 2013
9th joint meeting on foundations of software engineering. ACM, 2013,
pp. 147–157.<a href="#fnref2" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>S. Amasaki, “Cross-version defect prediction: use
historical data, crossproject data, or both?” Empirical Software
Engineering, pp. 1–23, 2020.<a href="#fnref3" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>S. McIntosh and Y. Kamei, “Are fix-inducing changes a
moving target? a longitudinal case study of just-in-time defect
prediction,” IEEE Transactions on Software Engineering, vol. 44, no. 5,
pp. 412–428, 2017.<a href="#fnref4" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>S. N.C., S. Majumder and T. Menzies, “Early Life Cycle
Software Defect Prediction. Why? How?,” 2021 IEEE/ACM 43rd International
Conference on Software Engineering (ICSE), Madrid, ES, 2021,
pp. 448-459, doi: 10.1109/ICSE43902.2021.00050.<a href="#fnref5"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Jill Larkin, John McDermott, Dorothea P. Simon, and
Herbert A. Simon. 1980. Expert and Novice Performance in Solving Physics
Problems. Science 208, 4450 (1980), 1335–1342.
DOI:http://dx.doi.org/10.1126/science.208.4450.1335
arXiv:http://science.sciencemag.org/content/208/4450/1335.full.pdf<a
href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>N. Cowan. 2001. The magical number 4 in short-term
memory: a reconsideration of mental storage capacity. Behav Brain Sci
24, 1 (Feb 2001), 87–114.<a href="#fnref7" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>George A Miller. 1956. The magical number seven, plus or
minus two: some limits on our capacity for processing information.
Psychological review 63, 2 (1956), 81.<a href="#fnref8"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>Jill Larkin, John McDermott, Dorothea P. Simon, and
Herbert A. Simon. 1980. Expert and Novice Performance in Solving Physics
Problems. Science 208, 4450 (1980), 1335–1342.
DOI:http://dx.doi.org/10.1126/science.208.4450.1335
arXiv:http://science.sciencemag.org/content/208/4450/1335.full.pdf<a
href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Recently, Ma et al. [^wei14] used evidence from
neuroscience and functional MRIs to argue that STM capacity might be
better measured using other factors than “number of items”. But even
they conceded that “the concept of a limited (STM) has considerable
explanatory power for behavioral data”.<a href="#fnref10"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>Susan Wiedenbeck, Vikki Fix, and Jean Scholtz. 1993.
Characteristics of the mental representations of novice and expert
programmers: an empirical study. International Journal of Man-Machine
Studies 39, 5 (1993), 793–812.<a href="#fnref11" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>Valerdi, Ricardo. “Heuristics for systems engineering
cost estimation.” IEEE Systems Journal 5.1 (2010): 91-98.<a
href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>Kington, Alison. “Defining Teachers’ Classroom
Relationships.” (2009).
https://eprints.worc.ac.uk/1885/1/Kington%202009.pdf<a href="#fnref13"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>Easterby-Smith, Mark. “The Design, Analysis and
Interpretation of Repertory Grids.” Int. J. Man Mach. Stud. 13 (1980):
3-24.<a href="#fnref14" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>Helen M. Edwards, Sharon McDonald, S. Michelle Young,
The repertory grid technique: Its place in empirical software
engineering research, Information and Software Technology, Volume 51,
Issue 4, 2009, Pages 785-798, ISSN 0950-5849,<a href="#fnref15"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p>Austin PC, Steyerberg EW. Events per variable (EPV) and
the relative performance of different strategies for estimating the
out-of-sample validity of logistic regression models. Stat Methods Med
Res. 2017 Apr;26(2):796-808. doi: 10.1177/0962280214558972. Epub 2014
Nov 19. PMID: 25411322; PMCID: PMC5394463.<a href="#fnref16"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p>Peduzzi P, Concato J, Kemper E, Holford TR, Feinstein
AR. A simulation study of the number of events per variable in logistic
regression analysis. J Clin Epidemiol. 1996 Dec;49(12):1373-9. doi:
10.1016/s0895-4356(96)00236-3. PMID: 8970487.<a href="#fnref17"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p>Alvarez, L., &amp; Menzies, T. (2023). Don’t Lie to Me:
Avoiding Malicious Explanations With STEALTH. IEEE Software, 40(3),
43-53.<a href="#fnref18" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p>Zhu, X., Vondrick, C., Fowlkes, C.C. et al. Do We Need
More Training Data?. Int J Comput Vis 119, 76–92 (2016).
https://doi-org.prox.lib.ncsu.edu/10.1007/s11263-015-0812-2<a
href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20"><p>Menzies, T., Turhan, B., Bener, A., Gay, G., Cukic, B.,
&amp; predictors. In Proceedings of the 4th international workshop on
Predictor models in software engineering (pp. 47-54).<a href="#fnref20"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21"><p>J. Nam, W. Fu, S. Kim, T. Menzies and L. Tan,
“Heterogeneous Defect Prediction,” in IEEE Transactions on Software
Engineering, vol. 44, no. 9, pp. 874-896, 1 Sept. 2018, doi:
10.1109/TSE.2017.2720603.<a href="#fnref21" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn22"><p>Hamlet, Richard G. “Probable correctness theory.”
Information processing letters 25.1 (1987): 17-25.<a href="#fnref22"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn23"><p><a
href="https://github.com/timm/ezr/tree/main/data">github.com/timm/ezr/tree/main/data</a><a
href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn24"><p>Udandarao, V., Prabhu, A., Ghosh, A., Sharma, Y., Torr,
P.H., Bibi, A., Albanie, S. and Bethge, M., 2024. No” zero-shot” without
exponential data: Pretraining concept frequency determines multimodal
model performance. arXiv preprint arXiv:2404.04125.<a href="#fnref24"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn25"><p><a
href="https://docs.google.com/document/d/1dF4GePCf04IW5uZnRSGQXRlzo5VyD5u0PQ3hfy-Zd6Q/edit">docs.google.com/document/d/1dF4GePCf04IW5uZnRSGQXRlzo5VyD5u0PQ3hfy-Zd6Q/edit</a>.<a
href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn26"><p><a
href="https://docs.google.com/document/d/1dF4GePCf04IW5uZnRSGQXRlzo5VyD5u0PQ3hfy-Zd6Q/edit">docs.google.com/document/d/1dF4GePCf04IW5uZnRSGQXRlzo5VyD5u0PQ3hfy-Zd6Q/edit</a>.<a
href="#fnref26" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn27"><p><a
href="https://www.linkedin.com/pulse/data-centers-its-environmental-impacts-i%C5%9F%C4%B1l-durmu%C5%9F-q5xvf/">www.linkedin.com/pulse/data-centers-its-environmental-impacts-i%C5%9F%C4%B1l-durmu%C5%9F-q5xvf/</a><a
href="#fnref27" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn28"><p>M. Easterby-Smith, Design, analysis and interpretation
of repertory grids, International Journal of Man-Machine Studies, 13(1),
1980, 3-24,<a href="#fnref28" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li
id="fn29"><p>www.quora.com/How-many-data-points-are-enough-for-linear-regression<a
href="#fnref29" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn30"><p>Zhu, X., Vondrick, C., Fowlkes, C.C. et al. Do We Need
More Training Data?. Int J Comput Vis 119, 76–92 (2016).
https://doi-org.prox.lib.ncsu.edu/10.1007/s11263-015-0812-2<a
href="#fnref30" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn31"><p>Menzies, T., Turhan, B., Bener, A., Gay, G., Cukic, B.,
&amp; predictors. In Proceedings of the 4th international workshop on
Predictor models in software engineering (pp. 47-54).<a href="#fnref31"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn32"><p>J. Nam, W. Fu, S. Kim, T. Menzies and L. Tan,
“Heterogeneous Defect Prediction,” in IEEE Transactions on Software
Engineering, vol. 44, no. 9, pp. 874-896, 1 Sept. 2018, doi:<a
href="#fnref32" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn33"><p>My personnel reading of Richard G. Hamlet. 1987.
Probable correctness theory. Inf. Process. Lett. 25, 1 (20 April 1987),
17–25. https://doi.org/10.1016/0020-0190(87)90088-3<a href="#fnref33"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
